# è‡ªåŠ¨æ–‡çŒ®ç»¼è¿°åŠ©æ‰‹

![Language: Python](https://img.shields.io/badge/Language-Python-blue?logo=python) ![Subject: CS/ML/AI](https://img.shields.io/badge/Subject-CS/ML/AI-yellowgreen) ![Model: Qwen-3](https://img.shields.io/badge/Model-Qwen--3-ff69b4)

å¤§å®¶å¥½ğŸ¤—æˆ‘åšäº†ä¸€ä¸ªé›†æˆ AI åŠŸèƒ½çš„è‡ªåŠ¨æ–‡çŒ®ç»¼è¿°åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®å‡ ä¸ªå…³é”®è¯æˆ–å‚è€ƒæ–‡çŒ®è‡ªåŠ¨ç”Ÿæˆä¸€ä»½æ–‡çŒ®æ¸…å•å¹¶è‡ªåŠ¨æ•´ç†/ä¿å­˜ AI ç”Ÿæˆçš„æ–‡çŒ®æ€»ç»“ã€‚è€Œä¸”æ›´é‡è¦çš„æ˜¯ï¼šä½ å¯ä»¥è‡ªç”±åœ°å¾€é‡Œé¢åŠ å…¥ä½ éœ€è¦çš„åŠŸèƒ½ï¼Œå› ä¸ºæ•´ä¸ªé¡¹ç›®çš„ä»£ç éå¸¸ç®€å•ï¼ˆ~200è¡Œï¼‰ã€‚å½“å‰ç‰ˆæœ¬çš„ä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š

1. **ï¼ˆè·å–å…ƒæ•°æ®ï¼‰** æˆ‘ä»¬ç»´æŠ¤äº†ä¸€ä»½ä» Hugging Face ä¸Šè·å–çš„â€œå†å±ŠAIä¼šè®®æ¥æ”¶è®ºæ–‡æ¸…å•â€ã€‚åŸºäºè¿™ä»½æ¸…å•ï¼Œæˆ‘ä»¬å°†æ¥æ”¶è®ºæ–‡çš„å…ƒä¿¡æ¯ï¼ˆåŒ…æ‹¬é¢˜ç›®ã€ä½œè€…ã€æ‘˜è¦ã€PDFã€å…³é”®è¯ï¼‰æ¸…æ´—å¹¶æ•´ç†æˆ JSON æ ¼å¼ï¼Œæ–¹ä¾¿åç»­è‡ªå®šä¹‰åˆ†æã€‚
2. **ï¼ˆåŸºäºè§„åˆ™çš„åˆç­›ï¼‰** æˆ‘ä»¬åŸºäºå…³é”®è¯åŒ¹é…ã€æ‘˜è¦çš„ç›¸ä¼¼åº¦å’Œä½œè€…å…³ç³»ç½‘åˆ†æç­‰æ–¹å¼å¯¹æ–‡ç« è¿›è¡Œåˆç­›ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜
3. **ï¼ˆAI/Agentic åŠŸèƒ½ï¼‰** å¯¹äºåˆç­›è¿‡åçš„æ–‡ç« ï¼Œæˆ‘ä»¬ä½¿ç”¨ AI åŠŸèƒ½ä¼š PDF å…¨æ–‡è¿›è¡Œæ€»ç»“ï¼ˆè¾“å‡ºä¸º`markdown`æ ¼å¼ï¼‰ï¼Œå¹¶å¯¹æ€»ç»“è¿‡åçš„æ–‡ç« è¿›è¡Œè¿›ä¸€æ­¥æ¯”å¯¹ï¼Œä»è€Œå¾—å‡ºä¸€ä»½

## å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## ä½¿ç”¨æ–¹æ³•

### æ–¹æ³• 1: å•ä¸ª URL è·å–æ•°æ®

```python
from fetch_huggingface import fetch_huggingface_data, save_to_json

# ä» dataset è·å–æ•°æ®
url = "https://huggingface.co/datasets/DeepNLP/NIPS-2022-Accepted-Papers"
data = fetch_huggingface_data(url)

# ä¿å­˜ä¸º JSON æ–‡ä»¶
save_to_json(data, "output.json")

# æˆ–è€…ç›´æ¥ä½¿ç”¨ JSON
import json
json_str = json.dumps(data, ensure_ascii=False, indent=2)
print(json_str)
```

### æ–¹æ³• 2: æ‰¹é‡è·å–æ‰€æœ‰ URLs çš„æ•°æ®

```bash
# è·å–æ‰€æœ‰ URLs çš„æ•°æ®
python data/batch_fetch.py

# æŒ‡å®š dataset çš„ splitï¼ˆå¦‚ 'train', 'test'ï¼‰
python data/batch_fetch.py train
```

æ‰¹é‡å¤„ç†ä¼šåœ¨ `output/` ç›®å½•ä¸‹ç”Ÿæˆï¼š
- æ¯ä¸ª URL å¯¹åº”çš„ JSON æ–‡ä»¶ï¼ˆå¦‚ `neurips_2022.json`ï¼‰
- `summary.json` æ±‡æ€»æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰å¤„ç†ç»“æœ

### æ–¹æ³• 3: åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from fetch_huggingface import fetch_huggingface_data
import json

# è·å– dataset æ•°æ®
dataset_url = "https://huggingface.co/datasets/DeepNLP/NIPS-2022-Accepted-Papers"
result = fetch_huggingface_data(dataset_url)

# è®¿é—®æ•°æ®
print(f"æ•°æ®ç±»å‹: {result['type']}")
print(f"æ•°æ®æ¡æ•°: {result['count']}")
print(f"å‰ 3 æ¡æ•°æ®:")
for item in result['data'][:3]:
    print(json.dumps(item, ensure_ascii=False, indent=2))
```

## å‡½æ•°è¯´æ˜

### `fetch_huggingface_data(url, split=None)`

ä» Hugging Face URL è·å–æ•°æ®ã€‚

**å‚æ•°:**
- `url`: Hugging Face dataset æˆ– space çš„ URL
- `split`: å¯¹äº datasetsï¼Œå¯ä»¥æŒ‡å®šè¦è·å–çš„åˆ†å‰²ï¼ˆå¦‚ 'train', 'test'ï¼‰ï¼Œé»˜è®¤ä¸º Noneï¼ˆè·å–ç¬¬ä¸€ä¸ªå¯ç”¨åˆ†å‰²ï¼‰

**è¿”å›:**
- åŒ…å«æ•°æ®çš„å­—å…¸ï¼Œå¯ä»¥è½¬æ¢ä¸º JSON

**ç¤ºä¾‹:**
```python
# è·å–æ•´ä¸ª dataset
data = fetch_huggingface_data("https://huggingface.co/datasets/...")

# è·å–ç‰¹å®š split
data = fetch_huggingface_data("https://huggingface.co/datasets/...", split="train")
```

### `save_to_json(data, output_file, indent=2)`

å°†æ•°æ®ä¿å­˜ä¸º JSON æ–‡ä»¶ã€‚

**å‚æ•°:**
- `data`: è¦ä¿å­˜çš„æ•°æ®å­—å…¸
- `output_file`: è¾“å‡ºæ–‡ä»¶è·¯å¾„
- `indent`: JSON ç¼©è¿›ç©ºæ ¼æ•°ï¼ˆé»˜è®¤ 2ï¼‰

## æ³¨æ„äº‹é¡¹

1. **Datasets**: ä½¿ç”¨ `datasets` åº“ç›´æ¥åŠ è½½ï¼Œæ”¯æŒæ‰€æœ‰ Hugging Face datasets
2. **Spaces**: é€šè¿‡ Hugging Face API è·å– space ä¿¡æ¯ï¼Œå¦‚æœ space å…³è”äº† datasetï¼Œä¼šå°è¯•è·å–å…³è”çš„æ•°æ®
3. **ç½‘ç»œè¿æ¥**: éœ€è¦èƒ½å¤Ÿè®¿é—® Hugging Face ç½‘ç«™
4. **æ•°æ®å¤§å°**: å¤§å‹æ•°æ®é›†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ä¸‹è½½å’Œå¤„ç†

## è¾“å‡ºæ ¼å¼

### Dataset è¾“å‡ºæ ¼å¼

```json
{
  "type": "dataset",
  "name": "username/dataset_name",
  "url": "https://huggingface.co/datasets/...",
  "data": [
    {
      "field1": "value1",
      "field2": "value2"
    }
  ],
  "count": 100
}
```

### Space è¾“å‡ºæ ¼å¼

```json
{
  "type": "space",
  "name": "username/space_name",
  "url": "https://huggingface.co/spaces/...",
  "space_info": {
    "id": "...",
    "title": "...",
    ...
  },
  "associated_dataset": "username/dataset_name",
  "data": [...]
}
```

